{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from utils import read_json\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = read_json('configs.json')\n",
    "DATA_PATH = configs[\"DATA_PATH\"]\n",
    "DB_PATH = configs['DB_PATH']\n",
    "files_list = [file for file in os.listdir(DATA_PATH) if os.path.isfile(os.path.join(DATA_PATH, file))]\n",
    "\n",
    "def read_pdf(files_list, data_path=DATA_PATH):\n",
    "    \n",
    "    for file in files_list:\n",
    "        loader = PyPDFLoader(data_path + file)\n",
    "        content = loader.load()\n",
    "        whole_pdf = \"\"\n",
    "        for i in range(len(content)):\n",
    "            whole_pdf += content[i].page_content\n",
    "\n",
    "    whole_pdf = whole_pdf.replace('\\n', '')\n",
    "    return whole_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['民政-108內正1.pdf',\n",
       " '測量重劃-102內正17.pdf',\n",
       " '都市計畫-102內正22.pdf',\n",
       " '都市計畫-102內正30.pdf',\n",
       " '都市計畫-107內正2.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_108_1= read_pdf(files_list=[files_list[0]])\n",
    "pdf_102_17= read_pdf(files_list=[files_list[1]])\n",
    "pdf_102_22= read_pdf(files_list=[files_list[2]])\n",
    "pdf_102_30= read_pdf(files_list=[files_list[3]])\n",
    "pdf_107_2= read_pdf(files_list=[files_list[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model = 'llama3',\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(input, pattern):\n",
    "\n",
    "    # Search using the pattern\n",
    "    match = re.search(pattern, input, re.S)\n",
    "\n",
    "    if match:\n",
    "        result = match.group(1).strip()  # Use strip() to remove any leading/trailing whitespace\n",
    "        print(\"Extraction succeed.\")\n",
    "    else:\n",
    "        print(\"No match found\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction succeed.\n",
      "Extraction succeed.\n",
      "Extraction succeed.\n",
      "Extraction succeed.\n",
      "Extraction succeed.\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"案\\s*由：(.*?)參、事實與理由：\"\n",
    "pdf_108_1_reason = extract_content(pdf_108_1, pattern)\n",
    "pdf_102_17_reason = extract_content(pdf_102_17, pattern)\n",
    "pdf_102_22_reason = extract_content(pdf_102_22, pattern)\n",
    "pdf_102_30_reason = extract_content(pdf_102_30, pattern)\n",
    "pdf_107_2_reason = extract_content(pdf_107_2, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"你是一個專業的中文AI助手，用繁體中文回答所有的問題，即使問題是英文也要用中文回答\"\n",
    "        ), \n",
    "    HumanMessage(\n",
    "        content=f\"對以下文件作內容摘要整理:{pdf_108_1_reason},用繁體中文回覆\"\n",
    "    )\n",
    "]\n",
    "model_response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='以下為內容摘要整理：\\n\\n**問題：** 內政部制定殯葬管理條例時，未能務實通盤瞭解殯葬業之問題。\\n\\n**結果：** 導致相關之規範及函釋未盡周延，歷時 10多年竟未能對症化解爭議，滋生諸多民怨，確有怠失。\\n\\n**需求：** 根據法定程序，提案糾正問題，提高殯葬管理的實質性和效率。', response_metadata={'model': 'llama3', 'created_at': '2024-05-10T06:55:23.836676Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 39828089500, 'load_duration': 4187500, 'prompt_eval_count': 12, 'prompt_eval_duration': 2895808000, 'eval_count': 128, 'eval_duration': 36919362000}, id='run-1d44c357-6899-4628-9bef-5bb859c7fe54-0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"你是一個專業的中文AI助手，用繁體中文回答所有的問題，即使問題是英文也要用中文回答\"\n",
    "        ), \n",
    "    HumanMessage(\n",
    "        content=f\"閱讀以下內容並以三元組方式描述以上內容中的主體與關係:{pdf_108_1_reason} \\n 以繁體中文回答。\"\n",
    "    )\n",
    "]\n",
    "model_response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('三元組test.txt', 'w') as file:\n",
    "    file.write(model_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"你是一個專業的中文AI助手，用繁體中文回答所有的問題，即使問題是英文也要用中文回答\"\n",
    "        ), \n",
    "    HumanMessage(\n",
    "        content=f\"將以下三元組用（主體，關係，對象）json格式來撰寫:\\n{model_response.content} \\n 以繁體中文回答。\"\n",
    "    )\n",
    "]\n",
    "tuple_json = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are the three tuples written in JSON format with the labels (主體, 關係, 對象) and answers in Traditional Chinese:\\n\\n```\\n[\\n  {\\n    \"主體\": \"內政部 (Ministry of the Interior)\",\\n    \"關係\": \"為殯葬業的管理者，制定殯葬管理條例以規範殯葬業的運作。\",\\n    \"對象\": \"殯葬業\"\\n  },\\n  {\\n    \"主體\": \"殯葬業\",\\n    \"關係\": \"需要法制的預期和掌握，以便在運作中能夠順暢地運行。\",\\n    \"對象\": \"法制\"\\n  },\\n  {\\n    \"主體\": \"法制\",\\n    \"關係\": \"是內政部和殯葬業之間的關係，透過法制來規範殯葬業的運作。\",\\n    \"對象\": \"內政部, 殯葬業\"\\n  }\\n]\\n```'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_json.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('三元組_json_test.txt', 'w') as file:\n",
    "    file.write(tuple_json.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ./models/7B/llama-model.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/7B/llama-model.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# n_gpu_layers=-1, # Uncomment to use GPU acceleration\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# seed=1337, # Uncomment to set a specific seed\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# n_ctx=2048, # Uncomment to increase the context window\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m llm(\n\u001b[0;32m     10\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: Name the planets in the solar system? A: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Prompt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m       max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \u001b[38;5;66;03m# Generate up to 32 tokens, set to None to generate up to the end of the context window\u001b[39;00m\n\u001b[0;32m     12\u001b[0m       stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m# Stop generating just before the model would generate a new question\u001b[39;00m\n\u001b[0;32m     13\u001b[0m       echo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Echo the prompt back in the output\u001b[39;00m\n\u001b[0;32m     14\u001b[0m ) \u001b[38;5;66;03m# Generate a completion, can also call create_completion\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\sean.chang\\AppData\\Local\\anaconda3\\envs\\supervisor_llm\\lib\\site-packages\\llama_cpp\\llama.py:336\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_path \u001b[38;5;241m=\u001b[39m lora_path\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m _LlamaModel(\n\u001b[0;32m    339\u001b[0m     path_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_params, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[0;32m    340\u001b[0m )\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Model path does not exist: ./models/7B/llama-model.gguf"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"./models/7B/llama-model.gguf\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervisor_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
